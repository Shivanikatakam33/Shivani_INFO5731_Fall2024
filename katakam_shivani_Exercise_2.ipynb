{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivanikatakam33/Shivani_INFO5731_Fall2024/blob/main/katakam_shivani_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The interesting research question that i have in my mind is\n",
        "In response to significant international events like natural catastrophes, political upheavals, or sporting occasions, how do social media opinions fluctuate?\n",
        "\n",
        "Kind of data that has to be collected:\n",
        "So as to answer this question, i am supposed to collect the data from social media like twitter, reddit, facebook, instagram and so on.\n",
        "Data that i collect should include\n",
        "The posts from social media platfoms like twitter, facebook etc. mainly with important event keywords. These posts should be able to convey its positivesness or negativity.\n",
        "We should also collect the location of the users from their posts or may be from their profile. We have to collect the important information related to their posts like the time, name etc.\n",
        "\n",
        "The amount of data needed for analysis:\n",
        "We have to collect atleast 1000 posts from evey single area of interest across a minimum of 3 areas of interest so that the entire data becomes 1000*3 which equals to 3000 posts per event.\n",
        "The result of our dataset will be 3000 posts per event. This dataset varies with the number of events.\n",
        "\n",
        "Detailed steps for collecting and saving the data:\n",
        "* First, we have to identify the keywords and hashtags related to particular event. for example lets consider 2024 Paris Olympics. identify keywords related to this olympics that are majorly used in social media.\n",
        "*lets use twitter API to collect all the posts. make sure that the posts collected are from particular time period. create a library to classify the posts based on sentiment.\n",
        "*find out the location based on their profiles.\n",
        "*Store all the data in a CSV file and make sure that it has columns like time, event, location, text and sentiment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw textblob\n",
        "import praw\n",
        "from textblob import TextBlob\n",
        "import csv\n",
        "import time\n",
        "reddit = praw.Reddit(client_id='your_client_id',\n",
        "                     client_secret='your_client_secret',\n",
        "                     user_agent='your_user_agent')\n",
        "subreddits = ['worldnews', 'politics', 'sports', 'naturaldisasters']\n",
        "posts = []\n",
        "keywords = [\"Natural Disaster\", \"Political Event\", \"Sporting Event\"]\n",
        "for subreddit in subreddits:\n",
        "    print(f\"Fetching posts from: {subreddit}\")\n",
        "    try:\n",
        "        for submission in reddit.subreddit(subreddit).new(limit=500):\n",
        "            if any(keyword in submission.title for keyword in keywords):\n",
        "                posts.append([submission.title, submission.selftext, submission.created_utc])\n",
        "            time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching from {subreddit}: {e}\")\n",
        "with open('reddit_data.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Title\", \"Body\", \"Timestamp\", \"Sentiment\"])\n",
        "    for post in posts:\n",
        "        text = post[0] + \" \" + post[1]\n",
        "        analysis = TextBlob(text)\n",
        "        sentiment = analysis.sentiment.polarity\n",
        "        writer.writerow([post[0], post[1], post[2], sentiment])\n",
        "print(\"Reddit data collection completed!\")"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c7d74a-4270-4be8-df4f-edd5d28d8bee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts from: worldnews\n",
            "Error fetching from worldnews: received 401 HTTP response\n",
            "Fetching posts from: politics\n",
            "Error fetching from politics: received 401 HTTP response\n",
            "Fetching posts from: sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching from sports: received 401 HTTP response\n",
            "Fetching posts from: naturaldisasters\n",
            "Error fetching from naturaldisasters: received 401 HTTP response\n",
            "Reddit data collection completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a354fab9-26d7-4d1a-caad-cf76d7a4b614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collected and saved to semantic_scholar_articles.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "def fetch_papers(query, year_from, year_to, num_papers):\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "    papers = []\n",
        "    start = 0\n",
        "    while len(papers) < num_papers:\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'yearFilter': f'{year_from}-{year_to}',\n",
        "            'offset': start,\n",
        "            'limit': 100\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        data = response.json()\n",
        "        for paper in data.get('data', []):\n",
        "            papers.append({\n",
        "                'Title': paper.get('title', ''),\n",
        "                'Venue': paper.get('venue', ''),\n",
        "                'Year': paper.get('year', ''),\n",
        "                'Authors': ', '.join(author.get('name', '') for author in paper.get('authors', [])),\n",
        "                'Abstract': paper.get('abstract', '')\n",
        "            })\n",
        "        start += 100\n",
        "        if len(data.get('data', [])) < 100:\n",
        "            break\n",
        "    return papers\n",
        "\n",
        "# Parameters\n",
        "query = \"XYZ\"\n",
        "year_from = 2014\n",
        "year_to = 2024\n",
        "num_papers = 1000\n",
        "# Fetch papers\n",
        "papers = fetch_papers(query, year_from, year_to, num_papers)\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "# Save to CSV\n",
        "df.to_csv('semantic_scholar_articles.csv', index=False)\n",
        "print('Data collected and saved to semantic_scholar_articles.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "https://myunt-my.sharepoint.com/:x:/r/personal/shivanikatakam_my_unt_edu/Documents/run_results_Shivanikatakam%203.csv?d=wd02f3bbe26014d14b3210e914f09a0b5&csf=1&web=1&e=UiWmY2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Compared to other in class exercise, this is a bit tough. but i learned a lot. usually i am not aware of this web scraping before. but now i got to know how to collect the data from different\n",
        "websites using tools like parsehub and octoparse. i have used parsehub tool and collected imdb movie names, elease yearr, and also movie url and atings as well.actually i felt difficulty in using parseweb tool\n",
        "as i am not aware of it before but that tool had many tutorials which made me learn many new things in the tool."
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}